{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Opportunistic Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparing dependancies\n",
    "\n",
    "The coursierapi is necessary at import time because it is mandatory to include the maven repository from the local folder, this step will not be required after the release, but it will be needed if you want to modify the code and use the modified version.\n",
    "\n",
    "The piece of code below, add the location of the new MavenRepository that is located in the path **/maven/local/repository**; this is the same path use in the container stating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step imports the required modules to execute the code. All these packages come from the previous Maven Instalation\n",
    "\n",
    "The imported libraries are:\n",
    "\n",
    "Module | Java's | Scala's | Description\n",
    ":----- | -------------: | --------------: | :----------\n",
    "wayang-core | 8, 11 | 2.11, 2.12 | provides core data structures and the optimizer (required)\n",
    "wayang-basic | 8, 11 | 2.11, 2.12 | provides common operators and data types for your apps (recommended)\n",
    "wayang-api-scala-java | 8, 11 | 2.11, 2.12 | provides an easy-to-use Scala and Java API to assemble wayang plans (recommended)\n",
    "wayang-java | 8, 11 | 2.11, 2.12 | adapters for [Java Stream](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html) processing platforms\n",
    "wayang-spark | 8, 11 | 2.11, 2.12 | adapters for [Apache Spark](https://spark.apache.org) processing platforms\n",
    "wayang-flink | 8, 11 | 2.11, 2.12 | adapters for [Apache Flink](https://flink.apache.org) processing platforms\n",
    "hadoop-common | 8,11 | - | Hadoop-commons is required because the lack of the Environment Variable **HADOOP_HOME**\n",
    "log4j-core | 8,11 | - | Logggin library to manipulate the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j/2.14.0/log4j-2.14.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j/2.14.0/log4j-2.14.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.14.0/log4j-api-2.14.0.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.14.0/log4j-core-2.14.0.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                         \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                        \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.api._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.api.Configuration\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.api.WayangContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.ExecutionContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.function.FunctionDescriptor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.plugin.Plugin\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.core.util.{Tuple => WayangTuple, WayangCollections}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.java.Java\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.wayang.spark.Spark\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.ArrayList\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Arrays\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.{Collection => JavaCollection}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.List\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math.{exp, abs, max}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConversions._\n",
       "\n",
       "//Logging change the level to INFO\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.Level\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.logging.log4j.core.config.Configurator\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.thoughtworks.paranamer:paranamer:2.8`\n",
    "\n",
    "import $ivy.`org.apache.wayang::wayang-api-scala-java:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-core:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-basic:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang:wayang-java:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.wayang::wayang-spark:0.6.1-SNAPSHOT`\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:2.8.5`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-core:2.14.0`\n",
    "\n",
    "import org.apache.wayang.api._\n",
    "import org.apache.wayang.core.api.Configuration\n",
    "import org.apache.wayang.core.api.WayangContext\n",
    "import org.apache.wayang.core.function.ExecutionContext\n",
    "import org.apache.wayang.core.function.FunctionDescriptor\n",
    "import org.apache.wayang.core.plugin.Plugin\n",
    "import org.apache.wayang.core.util.{Tuple => WayangTuple, WayangCollections}\n",
    "import org.apache.wayang.java.Java\n",
    "import org.apache.wayang.spark.Spark\n",
    "import java.io.File\n",
    "import java.util.ArrayList\n",
    "import java.util.Arrays\n",
    "import java.util.{Collection => JavaCollection}\n",
    "import java.util.List\n",
    "import scala.math.{exp, abs, max}\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "//Logging change the level to INFO\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "Configurator.setRootLevel(Level.INFO);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we include all the classes that has been used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mTransform\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transform(var features: Int) \n",
    "    extends FunctionDescriptor.SerializableFunction[String, Array[Double]] {\n",
    "\n",
    "  override def apply(line: String): Array[Double] = {\n",
    "    val pointStr: Array[String] = line.split(\",\")\n",
    "    val point: Array[Double] = Array.ofDim[Double](features + 1)\n",
    "    for (i <- 0 until pointStr.length) {\n",
    "      point(i) = pointStr(i).toDouble\n",
    "    }\n",
    "    point\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mComputeLogisticGradient\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ComputeLogisticGradient\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], Array[Double]] {\n",
    "\n",
    "  var weights: Array[Double] = _\n",
    "\n",
    "  override def apply(point: Array[Double]): Array[Double] = {\n",
    "    val gradient: Array[Double] = Array.ofDim[Double](point.length)\n",
    "    var dot: Double = 0\n",
    "    for (j <- 0 until weights.length) dot += weights(j) * point(j + 1)\n",
    "    for (j <- 0 until weights.length)\n",
    "      gradient(j + 1) = ((1 / (1 + exp(-1 * dot))) - point(0)) * point(j + 1)\n",
    "    //counter for the step size required in the update\n",
    "    gradient(0) = 1\n",
    "    gradient\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.weights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSum\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sum\n",
    "    extends FunctionDescriptor.SerializableBinaryOperator[Array[Double]] {\n",
    "\n",
    "  override def apply(o: Array[Double], o2: Array[Double]): Array[Double] = {\n",
    "    val g1: Array[Double] = o\n",
    "    val g2: Array[Double] = o2\n",
    "    if (//samples came from one partition only\n",
    "        g2 == null) g1\n",
    "    if (//samples came from one partition only\n",
    "        g1 == null) g2\n",
    "    val sum: Array[Double] = Array.ofDim[Double](g1.length)\n",
    "    \n",
    "    //count\n",
    "    sum(0) = g1(0) + g2(0)\n",
    "    for (i <- 1 until g1.length) sum(i) = g1(i) + g2(i)\n",
    "    sum\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mWeightsUpdate\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WeightsUpdate\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], Array[Double]] {\n",
    "\n",
    "  var weights: Array[Double] = _\n",
    "\n",
    "  var current_iteration: Int = _\n",
    "\n",
    "  var stepSize: Double = 1\n",
    "\n",
    "  var regulizer: Double = 0\n",
    "\n",
    "  def this(stepSize: Double, regulizer: Double) = {\n",
    "    this()\n",
    "    this.stepSize = stepSize\n",
    "    this.regulizer = regulizer\n",
    "  }\n",
    "\n",
    "  override def apply(input: Array[Double]): Array[Double] = {\n",
    "    val count: Double = input(0)\n",
    "    val alpha: Double = (stepSize / (current_iteration + 1))\n",
    "    val newWeights: Array[Double] = Array.ofDim[Double](weights.length)\n",
    "    for (j <- 0 until weights.length) {\n",
    "      newWeights(j) = (1 - alpha * regulizer) * weights(j) - alpha * (1.0 / count) * input(\n",
    "          j + 1)\n",
    "    }\n",
    "    newWeights\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.weights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "    this.current_iteration = executionContext.getCurrentIteration\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mComputeNorm\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ComputeNorm\n",
    "    extends FunctionDescriptor.ExtendedSerializableFunction[Array[Double], (Double, Double)] {\n",
    "\n",
    "  var previousWeights: Array[Double] = _\n",
    "\n",
    "  override def apply(weights: Array[Double]): (Double, Double) = {\n",
    "    var normDiff: Double = 0.0\n",
    "    var normWeights: Double = 0.0\n",
    "    for (j <- 0 until weights.length) {\n",
    "      normDiff += abs(weights(j) - previousWeights(j))\n",
    "      normWeights += abs(weights(j))\n",
    "    }\n",
    "      \n",
    "    (normDiff, normWeights)\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.previousWeights = executionContext\n",
    "      .getBroadcast(\"weights\")\n",
    "      .iterator()\n",
    "      .next()\n",
    "      .asInstanceOf[Array[Double]]\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLoopCondition\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LoopCondition(var accuracy: Double, var max_iterations: Int)\n",
    "    extends FunctionDescriptor.ExtendedSerializablePredicate[JavaCollection[(Double, Double)]] {\n",
    "\n",
    "  private var current_iteration: Int = _\n",
    "\n",
    "  override def test(collection: JavaCollection[(Double, Double)]): Boolean = {\n",
    "    val input: (Double, Double) = WayangCollections.getSingle(collection)\n",
    "    println(\"Running iteration: \" + current_iteration)\n",
    "    (input._1 < accuracy * max(input._2, 1.0) || current_iteration > max_iterations)\n",
    "  }\n",
    "\n",
    "  override def open(executionContext: ExecutionContext): Unit = {\n",
    "    this.current_iteration = executionContext.getCurrentIteration\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSGDImpl\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * This class executes a stochastic gradient descent optimization on Rheem.\n",
    "  */\n",
    "class SGDImpl(plugins: Array[Plugin]) {\n",
    "\n",
    "  def apply(confFile: Configuration, datasetUrl: String, datasetSize: Int, features: Int, maxIterations: Int, accuracy: Double, sampleSize: Int): Array[Double] = {\n",
    "    // Initialize the builder.\n",
    "    val context = new WayangContext(confFile)\n",
    "    for (plugin <- this.plugins) {\n",
    "      context.withPlugin(plugin)\n",
    "    }\n",
    "    val planBuilder = new PlanBuilder(context)\n",
    "      \n",
    "    // Create initial weights.\n",
    "    val weights: List[Array[Double]] = Arrays.asList(Array.ofDim[Double](features))\n",
    "      \n",
    "    val weightsBuilder: DataQuanta[Array[Double]] =\n",
    "      planBuilder.loadCollection(weights).withName(\"init weights\")\n",
    "      \n",
    "    // Load and transform the data.\n",
    "    val transformBuilder: DataQuanta[Array[Double]] = planBuilder\n",
    "      .readTextFile(datasetUrl).withName(\"source\")\n",
    "      .mapJava(new Transform(features)).withName(\"transform\")\n",
    "      \n",
    "    // Do the SGD\n",
    "    val loop: DataQuanta[Array[Double]] = weightsBuilder.doWhileJava(\n",
    "      new LoopCondition(accuracy, maxIterations),\n",
    "      (w) => {\n",
    "        var newWeightsDataset: DataQuanta[Array[Double]] =\n",
    "          transformBuilder\n",
    "            .sample(sampleSize, datasetSize).withBroadcast(w, \"weights\")\n",
    "            .mapJava(new ComputeLogisticGradient()).withBroadcast(w, \"weights\").withName(\"compute\")\n",
    "            .reduceJava(new Sum()).withName(\"reduce\")\n",
    "            .mapJava(new WeightsUpdate()).withBroadcast(w, \"weights\").withName(\"update\")\n",
    "          \n",
    "        var convergenceDataset: DataQuanta[(Double, Double)] = \n",
    "                 newWeightsDataset.mapJava(new ComputeNorm()).withBroadcast(w, \"weights\")\n",
    "          \n",
    "        new WayangTuple(newWeightsDataset, convergenceDataset)\n",
    "      },\n",
    "      maxIterations\n",
    "    )\n",
    "      \n",
    "    WayangCollections.getSingleOrNull(loop.collect())\n",
    "    \n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val inputFile = new File(\"files/HIGGS.csv\").toURI().toString()\n",
    "val confFile = new Configuration(new File(\"files/wayang_sgd.properties\").toURI().toString())\n",
    "\n",
    "new SGDImpl(Array(Spark.basicPlugin, Java.basicPlugin)).apply(confFile, inputFile, 11000000, 28, 1000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  },
  "license": "Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.See the NOTICE file distributed with this work for additional information regarding copyright ownership.The ASF licenses this file to you under the Apache License, Version 2.0(the \"License\"); you may not use this file except in compliance with the License.You may obtain a copy of the License at http: //www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
